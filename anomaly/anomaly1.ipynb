{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matplotlib.rcParams['savefig.dpi'] = 144\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection, Session 1\n",
    "\n",
    "The first two sessions will be focused on anomaly detection in time-series data.\n",
    "\n",
    "**Time series** differ from other sources of data in that they are explicitly ordered.  The usual intent is to use past data to make predictions about the future, so only data from the past may be used to make a prediction.  For simplicity, we will ignore this restriction for most of these sessions, but will discuss it in the context of **online learning** towards the end.\n",
    "\n",
    "**Anomaly detection**, or novelty detection, is attempting to find data that look different from the majority of the data.  It is typically an **unsupervised learning** system.  By this, we mean that the anomalous data is not **labeled**.  We must detect it by learning what the normal data look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CitiBike Ridership Data\n",
    "\n",
    "We will be looking at ridership from the CitiBike bike sharing system.  The data are available [online](https://s3.amazonaws.com/tripdata/index.html).  The zip files should be loaded in to the `anomaly/tripdata/` directory.  The script `download.sh` will do this for you.\n",
    "\n",
    "Let's start by looking at an individual file.  Python's *zipfile* module will allow us to read data from those zip files without manually unzipping each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = zipfile.ZipFile('tripdata/201307-citibike-tripdata.zip', 'r')\n",
    "zf.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zf.read(zf.namelist()[0])\n",
    "print '\\n'.join(data.split('\\n')[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *pandas* module provides a `DataFrame` class for powerful manipulation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(zf.open(zf.namelist()[0]))\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above, we see that we can take a count of the number of trips in a day by the following steps\n",
    "1. Extracting the day of the trip from the `starttime` field.\n",
    "2. Grouping trips by day, and taking a count of how many trips happened on each day.\n",
    "\n",
    "Once we do that, we can look for days that had anomalously high or low ridership counts. Which leads us to...\n",
    "\n",
    "The Big Question\n",
    "===\n",
    "\n",
    "*In our dataset, on what days was ridership of citibike abnormally high or abnormally low?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get just the day of the start time.  String processing is a bit faster than turning everything into datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['starttime'].str.split(' ', 1).apply(lambda x: x[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all we have to do is count how many time each date occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['starttime'].str.split(' ', 1).apply(lambda x: x[0]).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the index to actual datetime objects for convenience later.  Let's wrap this all up in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_counts(fn):\n",
    "    zf = zipfile.ZipFile(fn, 'r')\n",
    "    df = pd.read_csv(zf.open(zf.namelist()[0]))\n",
    "    counts = df['starttime'].str.split(' ', 1).apply(lambda x: x[0]).value_counts()\n",
    "    if '-' in counts.index[0]:\n",
    "        counts.index = pd.to_datetime(counts.index, format='%Y-%m-%d')\n",
    "    else:\n",
    "        counts.index = pd.to_datetime(counts.index, format='%m/%d/%Y')\n",
    "    return counts.sort_index()\n",
    "\n",
    "load_counts('tripdata/201307-citibike-tripdata.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's do this for all of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "fns = glob.glob('tripdata/[0-9][0-9][0-9][0-9][0-9][0-9]-citibike-tripdata.zip')\n",
    "counts = pd.concat([load_counts(fn) for fn in sorted(fns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts.plot()\n",
    "plt.ylabel('Rides per day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from ipywidgets import interact\n",
    "\n",
    "f = datetime(2013, 7, 1)\n",
    "l = datetime(2016, 6, 1)\n",
    "def interact_count(first=0, last=(l - f).days):\n",
    "    counts.plot()\n",
    "    f_d = f + timedelta(days=first)\n",
    "    l_d = f + timedelta(days=last)\n",
    "    plt.xlim(f_d, l_d)\n",
    "\n",
    "interact(interact_count, \n",
    "         first=(0, (l - f).days, 1),\n",
    "         last=(0, (l-f).days, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detrending\n",
    "\n",
    "The first thing we notice is... seasonality. But we'll get back to that. I promise.\n",
    "\n",
    "Until then, let's focus on what we want to accomplish. In particular, we want to build a basis for what \"normal\" ridership is, so we can determine days where ridership was anomalous. To this end, we will build a model of the ridership. This is an example of **supervised machine learning**.\n",
    "\n",
    "In supervised machine learning, we have a $n \\times p$ **feature matrix** $X_{ji}$.  Each column corresponds to one of the $p$ features, and each row to a particular observation, out of $n$ total.  We also have a **label vector** $y_j$ of length $n$.  The goal is to develop a model $f$ that predicts the labels from the corresponding feature row; that is,\n",
    "\n",
    "$$ f(X_{j\\cdot}) \\approx y_j \\ . $$\n",
    "\n",
    "![Feature matrix](images/matrix.svg)\n",
    "\n",
    "When the labels are numerical, the problem is known as **regression**.  Then the labels represent different categories, the problem is one of **classification**.  Our problem, estimating counts, is a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "A basic, yet quite powerful, machine learning model is **linear regression**.  It is a linear model, meaning that\n",
    "\n",
    "$$ f(X_{j\\cdot}) = \\sum_i X_{ji} \\beta_i = (X \\cdot \\beta )_j \\ , $$\n",
    "\n",
    "for some $p$-vector $\\beta$.  Linear regression finds this vector by minimizing the MSE\n",
    "\n",
    "$$ \\frac 1 N \\left|X\\cdot\\beta - y\\right|^2 \\ . $$\n",
    "\n",
    "There is a closed-form solution:\n",
    "\n",
    "$$ \\hat\\beta = (X^T X)^{-1} X^T y \\ . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with that basis, let's model. Ridership has been growing with time. The growth doesn't appear to be linear, but quadratic might be a good fit. This presents a problem: how do we model a quadratic trend with linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 4 plots: \n",
    "# first, y = x^2\n",
    "# second, transform the y values by taking the sqrt (osquare the x values)\n",
    "# third, a linear regression fitting plot #2\n",
    "# last, that same linear regression fitting plot #1\n",
    "\n",
    "x = np.arange(100)\n",
    "y = (x ** 2) + (1500 * np.random.uniform( size=100))\n",
    "plt.scatter(x, y, label=\"our base data\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"y = x^2 (with random noise)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, label=\"base data\")\n",
    "bad_model = np.poly1d(np.polyfit(x, y, deg=1))\n",
    "plt.plot(x, bad_model(x), 'r-', label=\"bad regression\")\n",
    "plt.xlabel(\"linear regression with quadratic relationship (BAD)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is bad - do not try to model non-linear data linearly. Instead, transform your features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = x ** 2\n",
    "model = np.poly1d(np.polyfit(transformed, y, deg=1))\n",
    "plt.scatter(transformed, y, label=\"transformed data\")\n",
    "plt.plot(transformed, model(transformed), 'r-', label=\"much better regressoin\")\n",
    "plt.xlabel(\"x^2 (transformed)\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, label=\"base data\")\n",
    "plt.plot(x, model(transformed), 'r-', label=\"good, transformed regression\")\n",
    "plt.xlabel(\"base data, with transformed regression\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's engineer our first two features! \n",
    "1. $t$ - simply the date - we'll use number of days since day 0.\n",
    "2. $t^2$ - number of days since day 0, squared.\n",
    "\n",
    "Now, we have the following setup for our linear regression:\n",
    "\n",
    "**Inputs**\n",
    "- `t`\n",
    "- `t^2`\n",
    "\n",
    "**Outputs**\n",
    "- `count`\n",
    "\n",
    "\n",
    "**Model**\n",
    "\n",
    "`count = c +  a * t + b * t^2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn\n",
    "\n",
    "This particular transformation is a particularly simple example of **feature engineering**.  Feature engineering is where much of the work of machine learning is done, so libraries will provide mechanisms to assist this process.\n",
    "\n",
    "We will be using the *Scikit Learn* module for machine learning.  It provides many tools, but the core is two types of classes: **transformers** and **estimators**.  Transformers take in a feature matrix and return a transformed version:\n",
    "``` python\n",
    "class Transformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "  def fit(self, X, y=None):\n",
    "    # Learn about the data\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X):\n",
    "    return ... # The transformed features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import base\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadBackground(base.BaseEstimator, base.TransformerMixin):\n",
    "            \n",
    "    def fit(self, X, y=None):\n",
    "        self.X0 = X[0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        days = (X - self.X0).days\n",
    "        return np.c_[days, days**2]\n",
    "    \n",
    "# our base feature matrix is just the dates\n",
    "X = counts.index\n",
    "QuadBackground().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators have a `predict()` method that returns the model's prediction for the label of a particular row.\n",
    "``` python                                                 \n",
    "class Estimator(base.BaseEstimator, base.RegressorMixin):\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    # Learn about the data\n",
    "    return self\n",
    "    \n",
    "  def predict(self, X):\n",
    "    return ... # The predicted labels\n",
    "```\n",
    "We'll use a linear regression estimator provided by Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_trans = QuadBackground().fit(counts.index).transform(counts.index)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_trans, counts)\n",
    "plt.plot(counts.index, counts, counts.index, lr.predict(X_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well are we doing? A bit more theory\n",
    "\n",
    "Both regression and classification have a number of different metrics to gauge the effectiveness of a model.  The most commonly used metric for regression is the **mean-squared error** (MSE):\n",
    "\n",
    "$$ \\mbox{MSE} = \\frac 1 N \\sum_{j=1}^N \\left[ f(X_{j\\cdot}) - y_j \\right]^2 \\ . $$\n",
    "\n",
    "The MSE has units of $y^2$, which can make its size difficult to judge.  We often use the **root mean-squared error** (RMSE) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(metrics.mean_squared_error(counts, lr.predict(X_trans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still leaves us with the problem that we have no basis for whatever our RMSE is. How much better can we do? How much better are we doing than the dumbest possible prediction? A good baseline is the mean model, which has a MSE equal to the variance of the data.  We'll take the square root to look at RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(counts.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a smal improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Growth is often exponential. Use a transformer to model exponential, instead of quadratic, growth, as a feature. How do the two models compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting seasonality\n",
    "\n",
    "**Seasonality** is the tendency of time-series data to have underlying cycles.  We typically wish to remove these variations before undertaking further analysis.\n",
    "\n",
    "It's very clear we have underlying cycles. What do you think contributes to those, and how frequently do those cycles happen?\n",
    "\n",
    "Once you have some guesses, how do we pull out concrete information about cyclical behavior? The first tool we we look at is the **autocorrelation**.  For two random variables, $X$ and $Y$, the covariance is defined as\n",
    "\n",
    "$$ \\mbox{Cov}[X, Y] = E\\left[(X - E[X])(Y - E[Y])\\right] = E[XY] - E[X]E[Y] \\ .$$\n",
    "\n",
    "If there is no correlation between the random variables, the covariance is 0.  If the two random variables always return the same value,\n",
    "\n",
    "$$ \\mbox{Cov}[X, Y] = \\mbox{Cov}[X, X] = \\mbox{Var}[X] \\ . $$\n",
    "\n",
    "The autocovariance of a time-series signal is just the covariance of the signal with a time-lagged copy.  The autocorrelation normalizes this by the variance of the signal:\n",
    "\n",
    "$$ \\rho(X \\mid \\tau) = \\frac{\\mbox{Cov}[X_t, X_{t+\\tau}]}{\\mbox{Var[X]}} \\ . $$\n",
    "\n",
    "Pandas provides a built-in autocorrelation plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.tools.plotting.autocorrelation_plot(counts)\n",
    "plt.axvline(365, color='k', ls=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yearly cycle is clearly visible.  Zooming in, more detail becomes obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "last = (counts.index[-1] - counts.index[0]).days\n",
    "def change_axes(bottom=0, top=last):\n",
    "    pd.tools.plotting.autocorrelation_plot(counts)\n",
    "    plt.xlim(bottom, top)\n",
    "interact(change_axes, bottom=(0, last, 1), top=(0, last, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.tools.plotting.autocorrelation_plot(counts)\n",
    "plt.xlim(0,60)\n",
    "plt.axvline(7, color='k', ls=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fourier analysis** takes advantage of the fact that any signal can be written as the sum of sinusoids: \n",
    "\n",
    "$$ X_t = \\sum_\\nu A_\\nu \\sin(2\\pi\\nu t) + B_\\nu \\cos(2\\pi\\nu t) \\ . $$\n",
    "\n",
    "The **Fourier transform** is used to read out the coefficients $A_\\nu$ and $B_\\nu$ from the original signal.  If the signal is sampled, this process is known as the **discrete Fourier transform** (DFT).  The standard algorithm for doing this is the **fast Fourier transform** (FFT), which is implemented in the *numpy* module.\n",
    "\n",
    "Quite often, we are not concerned with the Fourier coefficients directly, but with the total power provided at frequency $\\nu$, $A_\\nu^2 + B_\\nu^2$.  This **power spectrum** is easily derived from the Fourier transform.\n",
    "\n",
    "(In practice, Fourier analysis uses complex exponentials instead of sines and cosines.  Instead of real coefficients $A_\\nu$ and $B_\\nu$, the FFT returns a single complex coefficient $C_\\nu$.  The contribution to the power spectrum is $|C_\\nu|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_x = 1.0 * np.linspace(0, 10, 1000)\n",
    "fft_y = np.sin(np.pi * fft_x) + np.sin(2.0 * np.pi * fft_x) # 2 components, frequency 1 and 1/2\n",
    "plt.plot(fft_x, fft_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = np.fft.fft(fft_y)\n",
    "plt.plot((1.0 * np.arange(len(fft)))/ 10.,  np.abs(fft) ** 2)\n",
    "plt.xlim(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fft_counts = np.fft.fft(counts - counts.mean())\n",
    "yrs = (counts.index[-1] - counts.index[0]).days / 365."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(1.0*np.arange(len(fft_counts)) / yrs, np.abs(fft_counts)**2)\n",
    "plt.xlabel('Freq (1/yrs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Fourier transform returns information on frequencies up to the sampling frequency (1/day, or 365/year), only the results up to half of that are valid.  This is due to the problem of **aliasing**.  In a sampled signal, you can not distinguish a signal with a frequency above half of the sampling rate, known as the **Nyquist freqency**, from a signal with a frequency below that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 10, 1000)\n",
    "ts = np.arange(0, 11)\n",
    "f = 0.65\n",
    "plt.plot(t, np.sin(2*np.pi * f * t), t, -np.sin(2*np.pi * (1 - f) * t))\n",
    "ml, sl, bl = plt.stem(ts, np.sin(2*np.pi * f * ts))\n",
    "plt.setp(ml, 'markerfacecolor', 'r')\n",
    "plt.setp(sl, 'color', 'r')\n",
    "plt.setp(bl, visible=False)\n",
    "plt.xticks(ts)\n",
    "plt.ylim(-2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in on the low-frequency components, we can clearly see the yearly cycle dominating all other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1.0*np.arange(len(fft_counts)) / yrs, np.abs(fft_counts)**2)\n",
    "plt.axis([0,3, 0, 5e13])\n",
    "plt.xlabel('Freq (1/yrs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing focus some more, we can cleary see the weekly cycle, at 52/year, as well as additional peaks at 12/year and 8/year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1.0*np.arange(len(fft_counts)) / yrs, np.abs(fft_counts)**2)\n",
    "plt.axis([0,100, 0, 1e12])\n",
    "plt.xlabel('Freq (1/yrs)')\n",
    "plt.axvline(365/7., color='k', ls=':')\n",
    "plt.axvline(12, color='k', ls=':')\n",
    "plt.axvline(8, color='k', ls=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to modelling\n",
    "\n",
    "We now have indications of monthly, weekly, yearly, and (perhaps) quarterly cycles. Our next model will be to consider only the yearly cycles:\n",
    "\n",
    "$$ f(t) = A \\sin\\frac{2\\pi t}{365} + B\\cos\\frac{2\\pi t}{365} + f_0 \\ . $$\n",
    "\n",
    "At first glance, it may appear that linear regression is not suitable, since the model is not linear in $t$.  However, just as above with our quadratic features, this is an easy feature transform to make.\n",
    "\n",
    "Instead, of considering $t$, let's we consider the $n\\times 2$ feature matrix\n",
    "\n",
    "$$ X = \\left[ \\sin\\frac{2\\pi t}{365}\\ \\ \\cos\\frac{2\\pi t}{365} \\right] \\ , $$\n",
    "\n",
    "we do actually have a linear model. This is to say, we're attempting to estimate the coefficients $A$ and $B$ from above with our linear regression. It's just another feature transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FourierComponents(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, period):\n",
    "        self.period = period\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.X0 = X[0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        dt = (X - self.X0).days * 2 * np.pi / self.period\n",
    "        return np.c_[np.sin(dt), np.cos(dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FourierComponents(365)\n",
    "X_trans = fc.fit(counts.index).transform(counts.index)\n",
    "plt.plot(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_trans, counts)\n",
    "plt.plot(counts.index, counts, counts.index, lr.predict(X_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling each transformer manually quickly gets tedious and error-prone.  Scikit Learn comes to the rescue with **pipelines**.  Pipelines take a series of transformers and (optionally) an estimator.  A pipeline acts as an estimator itself.  When it is fit, it fits the first transformer, transforms with the first transformer, uses that value to fit the second transformer, transforms with the second transformer, *etc.*, until finally fitting the estimator.  When predict is called on the pipeline, it sends the feature matrix through each of the transformers, before finally calling predict on the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('fourier', FourierComponents(365)),\n",
    "                 ('lr', LinearRegression())])\n",
    "pipe.fit(counts.index, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we just pass our basic feature set (in this case, the dates we're considering), and the pipeline handles the feature extraction. We can then predict without needing to worry about transformation... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(metrics.mean_squared_error(counts, pipe.predict(counts.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much easier! We can also see this model is performing significantly better than our first model, which captured growth as a factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines and combining models\n",
    "\n",
    "We got our first taste of how pipelines make our life easier above. Now, let's see how scikit-learn makes our life easier by allowing us to combine our seasonal model and our quadratic growth model.\n",
    "\n",
    "Previously, we had 2 models:\n",
    "\n",
    "$$ m_{yearly}(t) = A \\sin\\frac{2\\pi t}{365} + B\\cos\\frac{2\\pi t}{365} + f_0  $$\n",
    "\n",
    "$$ m_{growth}(t) = C \\cdot t + D \\cdot t^2 + g_0 $$\n",
    "\n",
    "Where the first model determines coefficients for $A$ and $B$, and the second for $C$ and $D$. We now want to combine those into one, larger model:\n",
    "\n",
    "$$ m_{yearly + growth}(t) = A \\sin\\frac{2\\pi t}{365} + B\\cos\\frac{2\\pi t}{365} + f_0  +  C \\cdot t + D \\cdot t^2 + g_0 $$\n",
    "\n",
    "(Note we technically only have one intercept term, which you can think of as some weighted average of $f_0 + g_0$ if you like).\n",
    "\n",
    "Scikit-learn lets us easily translate this math into code with something called a `FeatureUnion`: Basically, apply each transformer to the input and combine all columns that get returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "union = FeatureUnion([('fourier', FourierComponents(365)),\n",
    "                      ('growth', QuadBackground())])\n",
    "u = union.fit(counts.index).transform(counts.index)\n",
    "u[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('union', union),\n",
    "                 ('lr', LinearRegression())])\n",
    "pipe.fit(counts.index, counts.values)\n",
    "plt.plot(counts.index, counts, counts.index, pipe.predict(counts.index))\n",
    "np.sqrt(metrics.mean_squared_error(counts.values, pipe.predict(counts.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Account for the monthly seasonality.  Examine how ridership varies over the month.  Develop a model to account for this.  How much does this improve the RMSE?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "So far, the features we've extracted have been *numerical* in nature: they clearly exist along a scale, with a clear ordering and spatial positioning, and we think about processing them as numbers. \n",
    "\n",
    "We now consider the weekly cycle we saw.  If we group the results by day of the week, we can get a better feel for the cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = pd.DataFrame(\n",
    "    {'day': counts.index.dayofweek, 'count': counts.values}\n",
    ")\n",
    "day_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df.groupby('day').mean().plot(kind='bar')\n",
    "plt.xticks(range(7), ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weekly cycle is not particularly sinusoidal.  Instead of treating the day of the week as a continuous variable, we will treat it as a **categorical feature**.  Such features denote membership in a class, without any particular ordering of those classes.  Therefore, we do not encode them in a single feature, but we create a new feature for each category.  Each row gets a 1 in the column corresponding to its category and a 0 in all others.  This is known as **one-hot encoding** or **dummy variables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DayofWeek(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def day_vector(self, day):\n",
    "        v = np.zeros(7)\n",
    "        v[day] = 1\n",
    "        return v\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.stack(self.day_vector(d) for d in X.dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DayofWeek().transform(counts.index)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When used with linear regression, one-hot encoding produces per-category means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow = DayofWeek().fit_transform(counts.index)\n",
    "lr = LinearRegression(fit_intercept = False).fit(dow,  counts)\n",
    "day_df.groupby(\"day\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add these features to our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "union = FeatureUnion([('fourier', FourierComponents(365)),\n",
    "                      ('dayofweek', DayofWeek()),\n",
    "                      ('growth', QuadBackground())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('union', union),\n",
    "                 ('lr', LinearRegression(fit_intercept=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(counts.index, counts)\n",
    "np.sqrt(metrics.mean_squared_error(counts, pipe.predict(counts.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what a model is doing correctly, and what it's missing, it's useful to plot the **residual**, the difference between the actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(counts - pipe.predict(counts.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
