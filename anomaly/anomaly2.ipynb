{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matplotlib.rcParams['savefig.dpi'] = 144\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection, Session 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn import base\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_counts(fn):\n",
    "    zf = zipfile.ZipFile(fn, 'r')\n",
    "    df = pd.read_csv(zf.open(zf.namelist()[0]))\n",
    "    counts = df['starttime'].str.split(' ', 1).apply(lambda x: x[0]).value_counts()\n",
    "    if '-' in counts.index[0]:\n",
    "        counts.index = pd.to_datetime(counts.index, format='%Y-%m-%d')\n",
    "    else:\n",
    "        counts.index = pd.to_datetime(counts.index, format='%m/%d/%Y')\n",
    "    return counts.sort_index()\n",
    "\n",
    "fns = glob.glob('tripdata/[0-9][0-9][0-9][0-9][0-9][0-9]-citibike-tripdata.zip')\n",
    "counts = pd.concat([load_counts(fn) for fn in sorted(fns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierComponents(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, period):\n",
    "        self.period = period\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.X0 = X[0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        dt = (X - self.X0).days * 2 * np.pi / self.period\n",
    "        return np.c_[np.sin(dt), np.cos(dt)]\n",
    "\n",
    "class DayofWeek(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def day_vector(self, day):\n",
    "        v = np.zeros(7)\n",
    "        v[day] = 1\n",
    "        return v\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.stack(self.day_vector(d) for d in X.dayofweek)\n",
    "\n",
    "class QuadBackground(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.X0 = X[0]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        days = (X - self.X0).days\n",
    "        return np.c_[days, days**2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a slightly more complex model than we ended the previous session with.  In addition to the yearly Fourier term and weekly binning, we have added monthly, bianual, and 8/year Fourier components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union = FeatureUnion([('date', QuadBackground()),\n",
    "                      ('fourier-y', FourierComponents(365)),\n",
    "                      ('fourier-2', FourierComponents(365/2.)),\n",
    "                      ('fourier-m', FourierComponents(365/12.)),\n",
    "                      ('fourier-8', FourierComponents(365/8.)),\n",
    "                      ('dayofweek', DayofWeek())])\n",
    "pipe = Pipeline([('union', union),\n",
    "                 ('lr', LinearRegression())])\n",
    "pipe.fit(counts.index, counts.values)\n",
    "np.sqrt(metrics.mean_squared_error(counts, pipe.predict(counts.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fair improvement on our previous RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "residuals = counts - pipe.predict(counts.index)\n",
    "plt.plot(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## z-Score\n",
    "\n",
    "From the residuals plot above, we can fairly easily see which values are likely to be anomalous: those far from zero.  We can quantify this measure with some results from statistics.\n",
    "\n",
    "Let $X$ be a random variable described by a **probability distribution function** $p(x)$.  This means that the probability of $X$ being found between $a$ and $b$ is\n",
    "\n",
    "$$ \\int_a^b p(x)\\,dx \\ . $$\n",
    "\n",
    "There are a number of quantities that describe a distribution; two of the most basic are the mean,\n",
    "\n",
    "$$ \\mu \\equiv \\int x\\,p(x)\\,dx\\ , $$\n",
    "\n",
    "and the standard deviation,\n",
    "\n",
    "$$ \\sigma \\equiv \\left[ \\int (x - \\mu )^2\\,p(x)\\,dx \\right]^{1/2} \\ . $$\n",
    "\n",
    "The mean indicates the center of the distribution, while the standard deviation measures its width.\n",
    "\n",
    "If we have a measurement $y$ and a distribution with $\\mu$ and $\\sigma$, the **z-score** measures the distance of $y$ from the mean, normalized by the standard deviation:\n",
    "\n",
    "$$ z = \\frac{y - \\mu}\\sigma \\ . $$\n",
    "\n",
    "Many random variables follow the Gaussian, or normal, distribution.  This is the common bell curve, which is parameterized by its mean and standard deviation:\n",
    "\n",
    "$$ p(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left(\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right) \\ . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "x = np.linspace(-4, 4, 100)\n",
    "dist = stats.norm()\n",
    "cols = seaborn.color_palette()[::2]\n",
    "cols = [cols[1], cols[2], cols[0], cols[0], cols[2], cols[1]]\n",
    "plt.plot(x, dist.pdf(x))\n",
    "for xm in xrange(-2, 4):\n",
    "    x = np.linspace(xm-1, xm, 20)\n",
    "    plt.fill_between(x, dist.pdf(x), alpha=0.5, lw=0, facecolor=cols[xm+2])\n",
    "plt.xlabel(r'$(x - \\mu) / \\sigma$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Gaussian, there is 68% chance of a random variable lying within $1\\sigma$ of the mean (blue), a 95% chance of it lying withing $2\\sigma$ (blue and yellow), and a 99.7% chance of it lying within $3\\sigma$ (blue, yellow and red).  Thus, the higher the z-score of a value, the less likely it is to have been drawn from that distribution.\n",
    "\n",
    "Of course, different distributions have different shapes, so these probabilities are not universal.  Still, for single-peaked distributions, the general pattern holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "residuals.hist(bins=50)\n",
    "x = np.arange(-30000, 15000, 100)\n",
    "dist = stats.norm(scale=residuals.std())\n",
    "plt.plot(x, dist.pdf(x) * 45000 / 50. * len(residuals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are not distributed normally, but we can still use large (absolute) z-scores as an indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z = residuals / residuals.std()\n",
    "plt.plot(z)\n",
    "plt.ylabel('z-score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the distrbution of residuals is not symmetric about 0, we will consider different thresholds for positive and negative z values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z[z > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[z < -3.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving-Window Averages\n",
    "\n",
    "One thing to note about this results is that most of the anomalous counts are rather recent.  Looking at the residuals, we can see why: There seems to be more variation in the recent data.  This suggests that we should be looking for measures that compare the latest results only to contemporary data.\n",
    "\n",
    "This is the domain of **moving-window averages**.  Instead of calculating values over the whole of the domain, we do the calculation over a window.  As we move forward in time, this window moves with us.  There are a number of window shapes we could use, but we will start with a simple square window of the last $k$ measurements.  Pandas makes it easy to calculate the mean and standard deviation over those windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def plot_rolling(x):\n",
    "    \n",
    "    def func(window=50):\n",
    "        rolling = x.rolling(window=window)\n",
    "        plt.plot(x, lw=1)\n",
    "        mean = rolling.mean()\n",
    "        std = rolling.std()\n",
    "        plt.fill_between(mean.index, mean+std, mean-std, facecolor=cols[1], alpha=0.5)\n",
    "        plt.plot(rolling.mean())\n",
    "    \n",
    "    return func\n",
    "\n",
    "interact(plot_rolling(residuals), window=(5, 100, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use these rolling values for mean and standard deviation, we get the **moving z-score**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rolling_z(x, window):\n",
    "    roll_mean = x.rolling(window=window).mean().shift(1) # Don't include current data\n",
    "    roll_std = x.rolling(window=window).std().shift(1)\n",
    "    return (x - roll_mean) / roll_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_z(x):\n",
    "    return lambda window: plt.plot(rolling_z(x, window)) and None\n",
    "\n",
    "interact(plot_rolling_z(residuals), window=(5, 100, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the choice of window size will always reflect a balance between the need to respond to new baselines while still noticing anomalous behavior, we can get an estimate for the size of the window we should use from the autocorrelation of the residuals.  In this case, the strong correlation only lasts about 5 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.tools.plotting.autocorrelation_plot(residuals)\n",
    "plt.xlim(0,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_z(residuals, 5)[rolling_z(residuals, 5) > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_z(residuals, 5)[rolling_z(residuals, 5) < -10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Window\n",
    "\n",
    "A particularly simple windowing average is the **exponentially-weighted moving average** (EWMA).  The value at time $t$ is given by\n",
    "\n",
    "$$ E_t = \\alpha X_t + (1 - \\alpha) E_{t-1}\\ . $$\n",
    "\n",
    "This has the effect of previous measurements fade away by a factor of $1 - \\alpha$ each step.  Previous values are never entirely forgotten, but they fade away bit by bit.\n",
    "\n",
    "There are several ways to specify the parameter; here we use the **half-life**,\n",
    "\n",
    "$$ h = \\log_2 \\frac{1}{1-\\alpha} \\ . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ewm(x):\n",
    "    \n",
    "    def func(halflife=50):\n",
    "        rolling = x.ewm(halflife=halflife)\n",
    "        plt.plot(x, lw=1)\n",
    "        mean = rolling.mean()\n",
    "        std = rolling.std()\n",
    "        plt.fill_between(mean.index, mean+std, mean-std, facecolor=cols[1], alpha=0.5)\n",
    "        plt.plot(rolling.mean())\n",
    "    \n",
    "    return func\n",
    "\n",
    "interact(plot_ewm(residuals), halflife=(5, 100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ewm_z(x, halflife):\n",
    "    ewm_mean = x.ewm(halflife=halflife).mean().shift(1) # Don't include current data\n",
    "    ewm_std = x.ewm(halflife=halflife).std().shift(1)\n",
    "    return (x - ewm_mean) / ewm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ewm_z(x):\n",
    "    return lambda halflife: plt.plot(ewm_z(x, halflife)) and None\n",
    "\n",
    "interact(plot_ewm_z(residuals), halflife=(5, 100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_z(residuals, 5)[ewm_z(residuals, 5) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewm_z(residuals, 5)[ewm_z(residuals, 5) < -4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including windowed data in model\n",
    "\n",
    "Instead of adapting our metric to consider recent data, we can include that recent data in the model.  Here, we two features: the count from the previous day, and a five-day rolling average.  There is a little bit of work necessary to hook all the components of the pipeline together, but in then end there is just a linear regression being performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame({'counts': counts, 'previous': counts.shift(1).fillna(method='bfill'),\n",
    "                          'rolling': counts.rolling(window=5).mean().shift(1).fillna(method='bfill')}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IndexExtractor(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.index\n",
    "\n",
    "class ColumnExtractor(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_pipe = Pipeline([('index', IndexExtractor()),\n",
    "                      ('features', union)])\n",
    "all_union = FeatureUnion([('time_pipe', time_pipe),\n",
    "                          ('columns', ColumnExtractor(['previous', 'rolling']))])\n",
    "lr_pipe = Pipeline([('all_union', all_union),\n",
    "                    ('lr', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipe.fit(counts_df, counts_df['counts'])\n",
    "np.sqrt(metrics.mean_squared_error(counts_df['counts'], lr_pipe.predict(counts_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This models scores significantly better.  It is also much more stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_window = counts_df['counts'] - lr_pipe.predict(counts_df)\n",
    "z_window = residuals_window / residuals_window.std()\n",
    "plt.plot(z_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_window[z_window > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_window[z_window < -3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Learning\n",
    "\n",
    "So far, we have been doing only retrospective analysis of the data.  In practice, we want to analyze incoming data in real time.  While we could train a model on an initial set of data and use that to analyze the future input, this means we don't learn anything from that future data.  (Extrapolation is also very dangerous if you have a quadratic background term.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dates = pd.date_range('2013-07-01', '2023-07-01', freq='d')\n",
    "plt.plot(future_dates, pipe.predict(future_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can avoid retraining the model each time we get new data by using an **online** learning algorithm, which is able to update itself with new data.  An exponentially-weighted moving average can be thought of as a simple online learning algorithm.  We can update its value from only the current value and the new measurement.  (In contrast, a rolling window requires us to store all the values in the window and recalculate the mean each step.)\n",
    "\n",
    "Linear regression does not support online learning.  However, **stochastic gradient descent** does.  Like linear regression, this is also a linear model.  Instead of finding an exact solution with matrix operations, stochastic gradient descent goes through the data row by row, updating its parameters slightly each time to reduce an error function.  Thus, it naturally supports online learning.\n",
    "\n",
    "In Scikit Learn, estimators that support online learning do so through a `partial_fit()` method.  Unfortunately, the default `Pipeline` class does not support `partial_fit()`, so we implement it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PartialFitPipeline(Pipeline):\n",
    "    \n",
    "    def partial_fit(self, X, y):\n",
    "        # Assume that none of the transformers need to be fit\n",
    "        Xtrans = X\n",
    "        for _, step in self.steps[:-1]:\n",
    "            Xtrans = step.transform(Xtrans)\n",
    "        self.steps[-1][1].partial_fit(Xtrans, y)\n",
    "        return self\n",
    "\n",
    "sgd_pipe = PartialFitPipeline([('all_union', all_union),\n",
    "                               ('scaler', StandardScaler()),\n",
    "                               ('sgd', SGDRegressor())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an exponential weighted moving average, since it's easier to manage in online learning.  Stochastic gradient descent is sensitive to different scales of data, so we need to normalize the features coming in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_init = counts.iloc[:700]\n",
    "counts_online = counts.iloc[700:]\n",
    "counts_init_df = pd.DataFrame({'counts': counts_init,\n",
    "                               'previous': counts_init.shift(1).fillna(method='bfill'),\n",
    "                               'rolling': counts_init.ewm(halflife=5).mean().shift(1).fillna(method='bfill')}) \n",
    "sgd_pipe.fit(counts_init_df, counts_init_df['counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, we make a prediction for the incoming record.  Then we train the model on the new record and update for the next record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_count = counts_init_df['counts'].iloc[-1]\n",
    "alpha = 1 - 1./2**5\n",
    "ewm = counts_init_df['rolling'].iloc[-1]\n",
    "predictions = []\n",
    "coefs = []\n",
    "\n",
    "for date, count in counts_online.iteritems():\n",
    "    df = pd.DataFrame({'counts': count, 'previous': last_count, 'rolling': ewm}, index=[date])\n",
    "    predictions.append(sgd_pipe.predict(df))\n",
    "    sgd_pipe.partial_fit(df, df['counts'])\n",
    "    coefs.append(sgd_pipe.named_steps['sgd'].coef_.copy())\n",
    "    last_count = count\n",
    "    ewm = alpha * count + (1 - alpha) * ewmdw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(counts_online, label='data')\n",
    "plt.plot(counts_online.index, predictions, label='model')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(counts_online.index, np.concatenate(predictions) - counts_online.values, label='residuals')\n",
    "plt.legend(loc=4)\n",
    "np.sqrt(metrics.mean_squared_error(counts_online.values, np.concatenate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is adjusting the coefficients over time, as it continues to learn from the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "\n",
    "1. Add more lagged or windowed data to the model, and see if its performance can be improved.  Beware that, if too many features are added, there is a danger of \"overfitting\".  To see if this is happening, hold back some data as a \"test\" set, and compare the model's performance on the training and test sets.\n",
    "\n",
    "    We will discuss overfitting in more detail in the recommendation sessions.\n",
    "\n",
    "2. Perform anomaly detection on the temperature data from the Central Park weather station.  Do temperature anomalies tend to align with ridership anomalies?  Add features to the ridership model corresponding to the temperature anomaly.  How does the performace of this model compare to one that involves temperature directly?\n",
    "\n",
    "3. We have been looking at only aggregate data so far.  Instead, look at the ridership at a particular station, and try to detect anomalies in its data.  Do these anomalies line up wit the system-wide anomalies or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Changepoints\n",
    "\n",
    "The previous model does a good job at producing a stationary signal, but there appear to be occasional changes in the variance.  One tool to deal with this is **Bayesian changepoint detection**.\n",
    "\n",
    "[Bayesian changepoint detection](https://hips.seas.harvard.edu/files/adams-changepoint-tr-2007.pdf) assumes that we have a signal drawn i.i.d. from some distribution.  However, at some (randomly occuring) points in time, the parameters of that distribution change.  We keep a estimate of the probability that it has been $T$ steps from the last changepoint, for all values of $T$.  As each new measurement $X_t$ comes in, we update the probabilities for each length run, conditioned on the newest measurement.  (Therefore, we need to track the mostly-probable parameters for the distribution, given each length run.)\n",
    "\n",
    "The algorithm needs to know what the underlying distribution is, what the Bayesian priors of the parameters are, and what the expected distribution of run lengths is.  The implementation below uses a Gaussian distribution for the samples and a geometric distribution for the run lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Ryan P. Adams's Matlab code: http://hips.seas.harvard.edu/content/bayesian-online-changepoint-detection\n",
    "\n",
    "def bayes_changepoint(X, mu0, sigma0, lambda_):\n",
    "    T = len(X)\n",
    "    R = np.zeros([T+1, T+1])\n",
    "    R[0,0] = 1\n",
    "    \n",
    "    kappa0 = 1\n",
    "    alpha0 = 1\n",
    "    beta0 = sigma0**2 / 2\n",
    "\n",
    "    muT = np.array(mu0)\n",
    "    kappaT = np.array(kappa0)\n",
    "    alphaT = np.array(alpha0)\n",
    "    betaT = np.array(beta0)\n",
    "\n",
    "    mp_mean = np.zeros(T)\n",
    "    mp_std = np.zeros(T)\n",
    "\n",
    "    maxes = np.zeros(T+1)\n",
    "    for t in xrange(T):\n",
    "        xt = (X[t] - muT) / np.sqrt(betaT * (kappaT+1) / (alphaT * kappaT))\n",
    "        predprobs = stats.t.pdf(xt, 2 * alphaT)\n",
    "        H = np.ones(t+1) / lambda_\n",
    "\n",
    "        R[1:t+2, t+1] =  R[:t+1,t] * predprobs * (1 - H)\n",
    "        R[0,     t+1] = (R[:t+1,t] * predprobs * H).sum()\n",
    "        R[:,t+1] = R[:,t+1] / R[:,t+1].sum()  # Numerics\n",
    "\n",
    "        mp = R[:, t+1].argmax()\n",
    "\n",
    "        muT0 = np.r_[mu0, (kappaT * muT + X[t]) / (kappaT + 1)]\n",
    "        kappaT0 = np.r_[kappa0, kappaT + 1]\n",
    "        alphaT0 = np.r_[alpha0, alphaT + 0.5]\n",
    "        betaT0 = np.r_[beta0, betaT + (kappaT * (X[t] - muT)**2 / (2 * (kappaT + 1)))]\n",
    "\n",
    "        muT = muT0\n",
    "        kappaT = kappaT0\n",
    "        alphaT = alphaT0\n",
    "        betaT = betaT0\n",
    "\n",
    "        mp_mean[t] = muT[mp]\n",
    "        mp_std[t] = np.sqrt(betaT[mp] * (kappaT[mp] + 1) / (alphaT[mp] * kappaT[mp]))\n",
    "    \n",
    "    return R, mp_mean, mp_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, mp_mean, mp_std = bayes_changepoint(residuals_window, 0, residuals_window.std(), 1000)\n",
    "plt.imshow(np.log10(R), vmin=-3, origin='lower')\n",
    "plt.colorbar()\n",
    "plt.plot(residuals_window.values/100 + 900, lw=0.5)\n",
    "plt.plot(R.argmax(axis=0), alpha=0.5, c='y')\n",
    "plt.axis([0,len(residuals_window),0,len(residuals_window)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each point in time, we can find the most probable run length and the mean and standard deviation associated with that run.  This provides another way to calculate a moving z-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(residuals_window, lw=1)\n",
    "plt.fill_between(residuals_window.index, mp_mean+mp_std, mp_mean-mp_std, facecolor=cols[1], alpha=0.5)\n",
    "plt.plot(residuals_window.index, mp_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(residuals_window / mp_std)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "405a6ea1ea964b68b2d486f4cfccbdc5": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
